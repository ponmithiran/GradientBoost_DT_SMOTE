{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "03e2b419",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from scipy import stats\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE, BorderlineSMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "\n",
    "from numpy import mean\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "3508b4d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load dataset\n",
    "\n",
    "titanic = pd.read_csv(\"dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8a8a964f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Cleaning\n",
    "\n",
    "titanic['Y'] = np.where(titanic['Y'] == 1 , 0, 1)\n",
    "\n",
    "titanic.dropna(inplace=True)\n",
    "\n",
    "\n",
    "titanic['44'] = titanic['44'].astype(str)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "262bb117",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Shape:  (59959, 46)\n",
      "New Shape:  (59959, 46)\n",
      "New Shape:  (59959, 46)\n",
      "New Shape:  (59959, 46)\n",
      "New Shape:  (59937, 46)\n",
      "New Shape:  (59937, 46)\n",
      "New Shape:  (59937, 46)\n",
      "New Shape:  (59936, 46)\n",
      "New Shape:  (59918, 46)\n",
      "New Shape:  (59918, 46)\n",
      "New Shape:  (59918, 46)\n",
      "New Shape:  (59918, 46)\n",
      "New Shape:  (59918, 46)\n",
      "New Shape:  (59911, 46)\n",
      "New Shape:  (59911, 46)\n",
      "New Shape:  (59903, 46)\n",
      "New Shape:  (59903, 46)\n",
      "New Shape:  (59903, 46)\n",
      "New Shape:  (59903, 46)\n",
      "New Shape:  (59903, 46)\n",
      "New Shape:  (59903, 46)\n",
      "New Shape:  (59903, 46)\n",
      "New Shape:  (59900, 46)\n",
      "New Shape:  (59899, 46)\n",
      "New Shape:  (59899, 46)\n",
      "New Shape:  (59899, 46)\n",
      "New Shape:  (59898, 46)\n",
      "New Shape:  (59898, 46)\n",
      "New Shape:  (59898, 46)\n",
      "New Shape:  (59898, 46)\n",
      "New Shape:  (59898, 46)\n",
      "New Shape:  (59898, 46)\n",
      "New Shape:  (59898, 46)\n",
      "New Shape:  (59898, 46)\n",
      "New Shape:  (59898, 46)\n",
      "New Shape:  (59898, 46)\n",
      "New Shape:  (59898, 46)\n",
      "New Shape:  (59866, 46)\n",
      "New Shape:  (59866, 46)\n",
      "New Shape:  (59850, 46)\n",
      "New Shape:  (59660, 46)\n",
      "New Shape:  (59646, 46)\n",
      "New Shape:  (59643, 46)\n"
     ]
    }
   ],
   "source": [
    "# Outlier Removal\n",
    "\n",
    "titanic = titanic.reset_index()\n",
    "''' Detection '''\n",
    "columns = titanic.columns\n",
    "columns= columns[:-3]\n",
    "\n",
    "for col in columns: \n",
    "    # IQR\n",
    "    Q1 = np.percentile(titanic[col], 25,\n",
    "                       interpolation = 'midpoint')\n",
    "\n",
    "    Q3 = np.percentile(titanic[col], 75,\n",
    "                       interpolation = 'midpoint')\n",
    "    IQR = Q3 - Q1\n",
    "\n",
    "    # Upper bound\n",
    "    upper = np.where(titanic[col] >= (Q3+3*IQR))\n",
    "    # Lower bound\n",
    "    lower = np.where(titanic[col] <= (Q1-3*IQR))\n",
    "    \n",
    "\n",
    "    ''' Removing the Outliers '''\n",
    "    titanic.drop(upper[0], inplace = True)\n",
    "    titanic.drop(lower[0], inplace = True)\n",
    "\n",
    "    print(\"New Shape: \", titanic.shape)\n",
    "    titanic = titanic.reset_index(drop=True)\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "4d7dfb5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   index         1         2         3         4         5         6  \\\n",
      "0      0  3.988833  2.230952  2.234090  2.224613  2.225998  2.216250   \n",
      "1      1  3.974519  2.226723  2.250789  2.212954  2.213453  2.222252   \n",
      "2      2  4.108639  2.411715  2.410257  2.399034  2.419996  2.415526   \n",
      "3      3  4.461258  2.583675  2.572882  2.574881  2.557647  2.589007   \n",
      "4      4  4.152431  2.408509  2.400121  2.441874  2.428172  2.454793   \n",
      "5      5  4.202709  2.389834  2.407209  2.394249  2.399547  2.396865   \n",
      "6      6  4.115625  2.367207  2.366003  2.354787  2.364106  2.384145   \n",
      "7      7  4.127285  2.396199  2.371090  2.404679  2.382529  2.376507   \n",
      "8      8  4.035359  2.311777  2.284966  2.336921  2.326167  2.322340   \n",
      "9      9  4.197079  2.409826  2.412902  2.384033  2.389567  2.422198   \n",
      "\n",
      "          7         8         9  ...  Y  WS_SB_1001.0  WS_SB_2002.0  \\\n",
      "0  2.212547  2.211953  2.215073  ...  0             1             0   \n",
      "1  2.207449  2.228175  2.223283  ...  0             1             0   \n",
      "2  2.413483  2.425236  2.405966  ...  0             1             0   \n",
      "3  2.582219  2.601832  2.573456  ...  0             1             0   \n",
      "4  2.418309  2.429708  2.412108  ...  0             1             0   \n",
      "5  2.390059  2.404040  2.400031  ...  0             1             0   \n",
      "6  2.375361  2.372786  2.350204  ...  0             1             0   \n",
      "7  2.382658  2.383538  2.384297  ...  0             1             0   \n",
      "8  2.313406  2.320369  2.308278  ...  0             1             0   \n",
      "9  2.426594  2.420648  2.404346  ...  0             0             0   \n",
      "\n",
      "   WS_SB_3003.0  WS_SB_3004.0  ZONE_B  ZONE_C  ZONE_D  ZONE_E  ZONE_EE  \n",
      "0             0             0       0       0       0       0        1  \n",
      "1             0             0       0       0       0       1        0  \n",
      "2             0             0       0       1       0       0        0  \n",
      "3             0             0       0       0       1       0        0  \n",
      "4             0             0       0       0       1       0        0  \n",
      "5             0             0       0       0       1       0        0  \n",
      "6             0             0       0       1       0       0        0  \n",
      "7             0             0       0       0       1       0        0  \n",
      "8             0             0       0       0       0       1        0  \n",
      "9             1             0       0       0       1       0        0  \n",
      "\n",
      "[10 rows x 55 columns]\n"
     ]
    }
   ],
   "source": [
    "# Encoding Categorical variables\n",
    "\n",
    "sortbin = pd.get_dummies(titanic['44'], prefix='WS_SB')\n",
    "rad_zone= pd.get_dummies(titanic['45'], prefix='ZONE')\n",
    "\n",
    "titanic = titanic.join(sortbin)\n",
    "titanic = titanic.join(rad_zone)\n",
    "print(titanic.head(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "bea61f91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop categorical values already encoded\n",
    "\n",
    "titanic.drop(['index'], axis=1, inplace=True)\n",
    "titanic.drop(['44'], axis=1, inplace=True)\n",
    "titanic.drop(['45'], axis=1, inplace=True)\n",
    "\n",
    "\n",
    "y = titanic.Y.copy()\n",
    "\n",
    "X = titanic.drop(['Y'], axis=1)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ce5e9762",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NormalDataset():\n",
    "    def __init__(self, X, y,train=0, scale_data=True):\n",
    "        #if not torch.is_tensor(X) and not torch.is_tensor(y):\n",
    "      # Apply scaling if necessary\n",
    "            \n",
    "        if scale_data:\n",
    "            \n",
    "            if train==1:\n",
    "                X_scale=ct.fit_transform(X)\n",
    "\n",
    "            elif train==0:\n",
    "                X_scale=ct.transform(X)\n",
    "\n",
    "        self.X=X_scale.astype('float64')\n",
    "        self.y=y\n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        return self.X[i], self.y[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3806fa5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Class used for tuning threshold values for multiclass models\n",
    "\n",
    "class proxyModel():\n",
    "    def __init__(self, origin_model):\n",
    "        self.origin_model = origin_model\n",
    "\n",
    "    def predict_proba(self, x, threshold_list=None):\n",
    "        # get origin probability\n",
    "        ori_proba = self.origin_model.predict_proba(x)\n",
    "\n",
    "        # set default threshold\n",
    "        if threshold_list is None:\n",
    "            threshold_list = np.full(ori_proba[0].shape, 1)\n",
    "            print(threshold_list)\n",
    "\n",
    "        # get the output shape of threshold_list\n",
    "        output_shape = np.array(threshold_list).shape\n",
    "\n",
    "        # element-wise divide by the threshold of each classes\n",
    "        new_proba = np.divide(ori_proba, threshold_list)\n",
    "\n",
    "        # calculate the norm (sum of new probability of each classes)\n",
    "        norm = np.linalg.norm(new_proba, ord=1, axis=1)\n",
    "\n",
    "        # reshape the norm\n",
    "        norm = np.broadcast_to(np.array([norm]).T, (norm.shape[0],output_shape[0]))\n",
    "\n",
    "        # renormalize the new probability\n",
    "        new_proba = np.divide(new_proba, norm)\n",
    "\n",
    "        return new_proba\n",
    "\n",
    "    def predict(self, x, threshold_list=None):\n",
    "        return np.argmax(self.predict_proba(x, threshold_list), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "03a2773d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare test amd train dataset\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0, shuffle=True)\n",
    "\n",
    "\n",
    "\n",
    "columns = X.columns.tolist()\n",
    "\n",
    "columns = columns[:len(columns)-9]\n",
    "\n",
    "sc=StandardScaler()\n",
    "ct = ColumnTransformer([\n",
    "                     ('somename', sc, columns)\n",
    "                     ], remainder='passthrough')\n",
    "\n",
    "\n",
    "dataset = NormalDataset(X_train, y_train,1)\n",
    "test = NormalDataset(X_test, y_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "00db3d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use SMOTE for imbalanced dataset\n",
    "\n",
    "oversample = SMOTE(random_state=0)\n",
    "X_os, y_os = oversample.fit_resample(dataset.X, dataset.y)\n",
    "\n",
    "\n",
    "\n",
    "New_sample= np.hstack((X_os,y_os.values.reshape(-1,1)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35a47aeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform grid search on various hyper-parameters\n",
    "\n",
    "grid = { \n",
    "    'n_estimators': [50, 100],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_depth' : [6,8],\n",
    "    'criterion' :['friedman_mse', 'squared_error'],\n",
    "    'random_state' : [18]\n",
    "}\n",
    "model = GradientBoostingClassifier()\n",
    "\n",
    "\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=0)\n",
    "rf_cv = GridSearchCV(estimator=model, param_grid=grid, scoring='roc_auc', cv= cv)\n",
    "rf_cv.fit(X_os, y_os)\n",
    "\n",
    "\n",
    "rf_cv.best_params_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7cf1126b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GradientBoostingClassifier(learning_rate=0.05, max_depth=10,\n",
       "                           max_features=&#x27;sqrt&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier(learning_rate=0.05, max_depth=10,\n",
       "                           max_features=&#x27;sqrt&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "GradientBoostingClassifier(learning_rate=0.05, max_depth=10,\n",
       "                           max_features='sqrt')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#redefine model based on best known parameters \n",
    "\n",
    "model = GradientBoostingClassifier(n_estimators=100,learning_rate=0.05, max_depth=10,max_features='sqrt')\n",
    "model.fit(X_os, y_os)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "239e67b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "pd.DataFrame(test.X).to_csv('input.csv')\n",
    "\n",
    "#threshold_list= [0.35,98.63,0.15,0.87]\n",
    "\n",
    "#P_model=proxyModel(model)\n",
    "\n",
    "\n",
    "y_pred = pd.Series(np.where(model.predict_proba(test.X)[:,1] > 0.02, 1, 0))\n",
    "#y_pred=pd.DataFrame(P_model.predict(test.X, threshold_list))\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "z = pd.concat([y_test, y_pred], axis=1)\n",
    "z.columns = ['True', 'Prediction']\n",
    "z.to_csv('output.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "c2eaef52",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall CCX: [0.08476815 0.97826087]\n"
     ]
    }
   ],
   "source": [
    "#Print evaluation metrics\n",
    "\n",
    "print(\"Accuracy:\", metrics.accuracy_score(test.y, y_pred))\n",
    "\n",
    "print(\"Recall:\", metrics.recall_score(test.y, y_pred, average=None))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65fb218a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
